# LLM-Evaluation

LLM-Evaluation is a collection of tools and techniques designed to assess the performance and capabilities of large language models (LLMs). This repository provides a comprehensive suite for evaluating various aspects of LLMs, including accuracy, efficiency, robustness, and adaptability. It includes automated benchmarking scripts, detailed metric reports, and comparison utilities to help researchers and developers understand and improve their models. Whether you are working on developing new LLMs or refining existing ones, LLM-Evaluation offers essential resources for thorough and insightful evaluation.

## LLM-Evaluation Metrics

| **Metric Type**           | **Metric**                             | **Description**                                                                                                 | **Implementation Method**                                                                 | **Need Label Data** | **Need Context Data** | **Need LLM Model** | **Need Custom Model** |
|---------------------------|----------------------------------------|-----------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|---------------------|----------------------|-------------------|--------------------|
| **Statistical Scorers**   | BLEU                                   | Evaluates precision of n-gram matches between LLM output and expected output.                                    | Calculates geometric mean of n-gram matches, with brevity penalty if needed.              | Yes                 | No                   | No                | No                 |
|                           | ROUGE                                  | Evaluates recall by comparing n-gram overlap between LLM output and expected output.                             | Determines the proportion of n-grams in the reference that are present in the LLM output. | Yes                 | No                   | No                | No                 |
|                           | METEOR                                 | Evaluates both precision and recall of n-gram matches, adjusting for word order and synonyms.                    | Uses harmonic mean of precision and recall, with penalty for ordering discrepancies.      | Yes                 | No                   | No                | No                 |
|                           | F1 scores Evaluation               | Evaluates Q&A system using F1 score based on predicted and actual answers.                                       | Uses F1 score calculated from predicted and actual answers.                               | Yes                 | No                   | No                | No                 |
|                           | Levenshtein Distance                   | Calculates minimum number of single-character edits needed to change one text string into another.               | Useful for evaluating spelling corrections and precise alignment of characters.           | Yes                 | No                   | No                | No                 |
| **Model-Based Scorers**   | NLI Scorer                             | Classifies LLM output as logically consistent, contradictory, or unrelated to a given reference text.            | Uses Natural Language Inference models to classify entailment, contradiction, and neutrality. | Yes              | Yes                  | Yes               | No                 |
|                           | BLEURT                                 | Uses pre-trained models like BERT to score LLM outputs against expected outputs.                                 | Relies on quality and representativeness of training data.                                | Yes                 | Yes                  | Yes               | Yes                |
| **LLM-Evals**             | G-Eval                                 | Uses LLMs to evaluate LLM outputs based on chain of thoughts and a form-filling paradigm.                        | Generates evaluation steps and normalizes scores using output token probabilities.        | Yes                 | No                   | Yes               | No                 |
|                           | Prometheus                             | Fine-tuned LLM for evaluation, comparable to GPT-4.                                                              | Requires reference/example evaluation results and score rubric in the prompt.             | Yes                 | Yes                  | Yes               | Yes                |
| **Combined Scorers**      | BERTScore                              | Computes cosine similarity between contextual embeddings of words in reference and generated texts.              | Aggregates similarities to produce a final score.                                         | Yes                 | No                   | Yes               | No                 |
|                           | MoverScore                             | Uses Earth Mover’s Distance to compute minimal cost to transform word distribution in LLM output to the reference text. | Employs deeply contextualized word embeddings from pre-trained models like BERT.     | Yes                 | No                   | Yes               | No                 |
|                           | GPTScore                               | Uses conditional probability of generating target text as an evaluation metric.                                  | Evaluates quality based on the likelihood of generating a reference text.                 | Yes                 | No                   | Yes               | No                 |
|                           | SelfCheckGPT                           | Uses sampling-based approach to fact-check LLM outputs, assuming hallucinated outputs are not reproducible.       | Suitable for hallucination detection; evaluates consistency of sampled responses.         | No                  | No                   | Yes               | No                 |
|                           | QAG Score                              | Uses answers to close-ended questions to compute final metric score, leveraging LLMs’ reasoning capabilities.     | Extracts claims from LLM output and verifies against ground truth with yes/no questions.  | Yes                 | Yes                  | Yes               | No                 |
|                           | QnA Ada Similarity Evaluation          | Measures similarity using text-embedding-ada-002 model between predicted and actual answers.                     | Uses cosine similarity of Ada embeddings between predicted and actual answers.            | Yes                 | No                   | Yes               | No                 |
|                           | QnA Relevance Evaluation               | Measures how relevant the predicted answer is to the question using LLMs.                                         | Uses LLM to score relevance on a scale of 1 to 5.                                         | No                  | No                   | Yes               | No                 |
|                           | QnA Fluency Evaluation                 | Measures grammatical and linguistic correctness of the predicted answer.                                          | Uses LLM to score fluency on a scale of 1 to 5.                                           | No                  | No                   | Yes               | No                 |
|                           | QnA Coherence Evaluation               | Measures the coherence and overall quality of sentences in the predicted answer.                                  | Uses LLM to score coherence on a scale of 1 to 5.                                         | No                  | No                   | Yes               | No                 |
|                           | QnA Groundedness Evaluation            | Measures how well the predicted answer is grounded in the provided context.                                       | Uses LLM to score groundedness on a scale of 1 to 5 based on consistency with context.    | No                  | Yes                  | Yes               | No                 |
|                           | QnA Relevance Scores Pairwise Evaluation | Assigns relevance scores to answers based on match with the question and compares with baseline answers.          | Uses LLM to score relevance on a scale of 0 to 100 and determines win/lose against baseline. | No              | No                   | Yes               | No                 |
|                           | QnA GPT Similarity Evaluation          | Measures similarity between user-provided actual answer and LLM predicted answer.                                 | Uses LLM to score similarity on a scale of 1 to 5.                                        | Yes                 | No                   | Yes               | No                 |
| **RAG Metrics**           | Faithfulness                           | Evaluates factual alignment of LLM output with the retrieval context.                                             | Uses QAG Scorer to check claims against retrieval context, calculating the proportion of truthful claims. | No           | Yes                  | Yes               | No                 |
|                           | Answer Relevancy                       | Assesses relevancy of sentences in LLM output to the input, considering retrieval context.                        | Calculates proportion of relevant sentences using QAG.                                    | No                  | Yes                  | Yes               | No                 |
|                           | Contextual Precision                   | Measures relevancy of nodes in retrieval context.                                                                 | Determines proportion of relevant nodes in retrieval context to the input.                | No                  | Yes                  | Yes               | No                 |
|                           | Contextual Recall                      | Measures alignment of retrieved information with expected output.                                                 | Evaluates proportion of expected output attributable to retrieval context nodes.          | Yes                 | Yes                  | Yes               | No                 |
|                           | Contextual Relevancy                   | Proportion of sentences in retrieval context that are relevant to the input.                                      | Measures contextual relevancy directly.                                                   | No                  | Yes                  | Yes               | No                 |
| **Fine-Tuning Metrics**   | Hallucination                          | Evaluates proportion of hallucinated sentences in LLM output.                                                     | Uses SelfCheckGPT or NLI scorer with provided context as ground truth.                    | No                  | Yes                  | Yes               | No                 |
|                           | Toxicity                               | Assesses extent of offensive, harmful, or inappropriate language in text.                                         | Uses pre-trained models like Detoxify or G-Eval with custom criteria for evaluation.      | No                  | No                   | Yes               | No                 |
|                           | Bias                                   | Evaluates presence of political, gender, and social biases in text.                                                | Uses G-Eval with clear rubrics for in-context learning, accounting for subjective variations across different environments. | No                  | No                   | Yes               | No                 |
| **Use Case Specific Metrics** | Summarization                     | Evaluates factual alignment and inclusion of important information from original text in summary.                 | Uses QAG to calculate factual alignment and inclusion scores, taking the minimum of the two as the final summarization score. | Yes       | Yes                  | Yes               | No                 |
| **Classification Metrics** | Classification Accuracy Evaluation   | Measures performance of classification systems by comparing predicted and actual labels.                          | Compares predicted output with actual labels to determine accuracy.                        | Yes                 | No                   | No                | No                 |




